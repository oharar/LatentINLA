---
title: "Constrained-GLLVM-from-Scratch"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Constrained_GLLVM-from-Scratch}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Constrained GLLVMs: this currently almost works

## Introduction

In a standard GLLVM the latent variables only affect the residual covariance, so if there are a lot of covariates, we still have to estimate a lot of parameters. Here we fit a model where the covariates model the latent variables, and these then get projected (as a whole) onto the data.

Mathematically, we have

$$
g(E(Y_{ij}) = \beta_{0j} + \eta_{ij}
$$

for $i$ rows (= $i$ multivariate observations) and $j$ columns. Then 

$$
\eta_{ij} = k_i \gamma_j = (\mathbf{X}^T_{l,i} \mathbf{B} + \epsilon^T_i) \gamma_j
$$
i.e. the site scores are now regressed against the covariates.

This is more difficult to set up in INLA. As with the simpler GLLVM, we will use the trick of defining the first latent variable on the first species, and then copying it onto the others. But now the latent variable is a linear predictor, not a single value. Luckily, [that problem has been solved](https://www.r-inla.org/faq#h.pb4g1pwbmtli) by the INLA team. The trick is to create a new variable. So for the first column, and with one latent variable, we have

$$
g(E(Y_{i1}) = \sum_k {\beta_{k1} x_{ik}} + \epsilon_{i1} = \eta_{i1}
$$
i.e. it is a simple regression. Then we have to create another piece of "data":

$$
0 = \sum_k {\beta_{k1} x_{ik}} + \epsilon_{i1} - \theta_{i1} + \varepsilon_{i1}
$$
where we force $\varepsilon_{i1}$ to have a small variance (roughly, smaller than the MC error if we were fitting this with MCMC). This them means that $\theta_{i1} = \eta_{i1} + \varepsilon_{i1}$, and we can use `copy=` to move $\theta_{i1}$ around the model.


## The Code

```{r setup, message=FALSE}
library(LatentINLA)
library(INLA)

```

First, create a small data set. Too small to really do anything with other than see how it gets used.

```{r CreateData}
NRows <- 2
NCols <- 5 
NCovs <- 2
Y.mat <- matrix(1:(NRows*NCols), nrow=NRows)
X <- matrix(10+1:(NCovs*NRows), nrow=NRows)
nLVs <- 1
```

So there are `r NCols` columns, `r NRows` rows and `r NCovs` covariates. We will assume the data follow a Poisson distribution, and use 1 latent variable. 

First we format the data:

```{r FormatData}
(dat <- FormatDataFrameForLV(Y.mat))
```

Then we add an extra column for the latent variable. We will add it at the end of the data frame, because that will make the other coding a bit easier.

```{r AddLVCol}
# Add NA rows to data 
RowNAs <- matrix(NA, nrow=NRows, ncol=NCols, dimnames=list(NULL, colnames(dat)))
dat2 <- rbind(dat, RowNAs)
(dat3 <- cbind(dat2, LV = c(rep(NA, nrow(dat)), rep(0, NRows))))

```

Now we create the covariate data. We only need this for the first data column and the LV column (the last colunm):

```{r CreateCovaraites}
# Add intercept to covaraites
X.int <- cbind(1, X)
# repeat covariates for data
Cov.dat <- rbind(X.int, # For the first data column
                 matrix(NA, ncol=ncol(X.int), nrow=nrow(dat3)-2*nrow(X.int)), # other columns
                 X.int)  # For the LV
colnames(Cov.dat) <- c("Intercept", "X1", "X2")
Cov.dat
```

Next we need to create the latent variables. Because we are putting the LV first, we almost just use the code to create the LVs as before: it already removes the link between column 1 and the LV.

```{r CreateLVs}
# create LV vectors for data
LV <- CreateLVIndices(dat=cbind(L=rep(0, nrow(Y.mat)), Y.mat), nLVs=nLVs)
(LV <- LV[,-1])
```

Finally, we need some weights for the latent variable:

```{r CreateWeight}
(w <- c(rep(NA, nrow(LV) - nrow(Y.mat)), rep(-1, nrow(Y.mat))))
```

How does this all work?

- the data are regressed against column 1 of the data
- the data are copied onto the LV data these minus the LV are regressed against 0. So at this point the only information about the effects of the covariates come from the first column: the LV absorbs all of the other information
- the LV is then copied onto the other data columns, up to a proportionality constant (which is estimated, so is not really a constant).



Now we can write the formula. We add an intercept manually (we should really have pne per column)


```{r CreateFormula}
Data <- list(Y = dat3, Intercept = Cov.dat[,"Intercept"], 
             X1 = Cov.dat[,"X1"], X2 = Cov.dat[,"X2"],
             LV=LV, w=w)
  
Formula <- formula(
  Y ~ Intercept + X1 + X2 + 
    f(LV$L, w, model="iid", hyper = list(prec = list(initial = -6, fixed=TRUE))) + 
    f(LV$col3, copy = "LV$L", hyper = list(beta = list(fixed = FALSE))) + 
    f(LV$col4, copy = "LV$L", hyper = list(beta = list(fixed = FALSE))) + 
    f(LV$col5, copy = "LV$L", hyper = list(beta = list(fixed = FALSE))) + 
    f(LV$col6, copy = "LV$L", hyper = list(beta = list(fixed = FALSE))) - 1
)
```

And finally fit the model. We won't run this line, because INLA will hate it, thanks to the lack of data. An example which runs is below.

```{r FitModel, eval=FALSE}
# fit the model
toyLVmodel = inla(Formula, data=Data, 
                  family = c("gaussian", rep("poisson", ncol(Data$Y)-1)))
```

## An Example that Runs

Now to show that this runs, using a larger data set. Firstwe create some data. 

```{r CreateBiggerData}
  NRows <- 200
  nLVs <- 1
  NCol <- 20
  NCovs <- 2
  CovBeta <- c(-0.1, 0.7)
  ColEffs <- matrix(c(1,rnorm(nLVs*(NCol-1), 0, 0.5)), ncol=NCol)
  X <- matrix(rnorm(NRows*NCovs, 0, 1), nrow=NRows)
  LV.true <- X%*%CovBeta
  E.Y <- 1+LV.true%*%ColEffs

  Y.mat <- apply(E.Y, 2, function(e) rpois(length(e), exp(e)))

```


Net the data is (are? Fight it out among yourselves) formatted:

```{r ActuallyFormataModel}
dat <- FormatDataFrameForLV(Y.mat)
# Add NA rows to data 
RowNAs <- matrix(NA, nrow=NRows, ncol=NCol, dimnames=list(NULL, colnames(dat)))
dat2 <- rbind(dat, RowNAs)
dat3 <- cbind(dat2, LV = c(rep(NA, nrow(dat)), rep(0, NRows)))

# repeat covariates for data
X.int <- cbind(1, X)
Cov.dat <- rbind(X.int, 
                 matrix(NA, ncol=ncol(X.int), nrow=nrow(dat3)-2*nrow(X.int)), 
                 X.int)
colnames(Cov.dat) <- c("Intercept", "X1", "X2")

# create LV vectors for data, and weight
LV <- CreateLVIndices(dat=cbind(Y.mat, L=rep(0, nrow(Y.mat))), nLVs=nLVs)
LV <- LV[,-1]
names(LV)[length(names(LV))] <- "L"
w <- c(rep(NA, nrow(LV) - nrow(Y.mat)), rep(-1, nrow(Y.mat)))

Data <- data.frame(Intercept = Cov.dat[,"Intercept"],
  X1 = Cov.dat[,"X1"], X2 = Cov.dat[,"X2"],
             LV=LV, w=w)
Data$Y <- dat3
```

And finally we get to fit a model:

```{r ActuallyFitaModel}

Formula <- formula(
  paste0("Y ~ Intercept + X1 + X2 + ", 
    "f(LV.L, w, model='iid', hyper = list(prec = list(initial = -6, fixed=TRUE))) + ", 
    paste0("f(", names(Data)[grep("LV.col", names(Data))], 
           ", copy = 'LV.L', hyper = list(beta = list(fixed = FALSE)))",
    collapse=" + "), collapse = "")
)  

# fit the model
LVmodel = inla(Formula, data=Data, 
                  family = c("gaussian", rep("poisson", ncol(Data$Y)-1)))
summ <- summary(LVmodel)

fixed <- LVmodel$summary.fixed
roweffs <- LVmodel$summary.random[grep("\\.L$", names(LVmodel$summary.random))]

knitr::kable(fixed[,1:2], digits=3)

```

At the moment is isn't working well. Some digging is needed...

We can look at the column effects, which aren't great either:

```{r ColEffs}
Betas <- LVmodel$summary.hyperpar[grep("^Beta",
                                           rownames(LVmodel$summary.hyperpar)),]

ColScores <- rbind(c(1, 0, NA, NA, NA, 1), Betas)
rownames(ColScores) <- paste0("col", 1:nrow(ColScores))

plot(ColEffs, ColScores[,1], xlab="True column effects", 
     ylab="Estimated column effects")
segments(ColEffs, ColScores[,3], ColEffs, ColScores[,4])
abline(0,1)
```
